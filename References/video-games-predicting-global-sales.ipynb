{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2cf29928415605f259a3b21d37d23cdf656f75a7"
   },
   "outputs": [],
   "source": [
    "donut_chart(data[\"Platform\"])\n",
    "plt.title(\"Platforms\")\n",
    "plt.axis(\"equal\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1bc0207fd3e5cbb0172f9557febd4706ce4ff2ff"
   },
   "outputs": [],
   "source": [
    "data[\"Platform\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "783c5ab5a0fcf6248af17f3f4923d3f952427dad"
   },
   "source": [
    "There are too many different platforms and most of them represent a very small percent of games. I am going to group platforms to reduce the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "229c2321eac3f924dbbd7fa8565b40be9bdc5d8c",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "platforms = {\"Playstation\" : [\"PS\", \"PS2\", \"PS3\", \"PS4\"],\n",
    "             \"Xbox\" : [\"XB\", \"X360\", \"XOne\"], \n",
    "             \"PC\" : [\"PC\"],\n",
    "             \"Nintendo\" : [\"Wii\", \"WiiU\"],\n",
    "             \"Portable\" : [\"GB\", \"GBA\", \"GC\", \"DS\", \"3DS\", \"PSP\", \"PSV\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "26c49f52fc31e77962653ece882d44b3b192e007",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_group_label(x, groups=None):\n",
    "    if groups is None:\n",
    "        return \"Other\"\n",
    "    else:\n",
    "        for key, val in groups.items():\n",
    "            if x in val:\n",
    "                return key\n",
    "        return \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4ea2c6e47d1f83081019b48ca80fd51285111900"
   },
   "outputs": [],
   "source": [
    "data[\"Grouped_Platform\"] = data[\"Platform\"].apply(lambda x: get_group_label(x, groups=platforms))\n",
    "donut_chart(data[\"Grouped_Platform\"])\n",
    "plt.title(\"Groups of platforms\")\n",
    "plt.axis(\"equal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e5bd3c9e02f5c3e2c5d64caf147647ee9a62b10"
   },
   "source": [
    "Looks much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "878baca7779ad035438d1386e1f55f0dbffb1496"
   },
   "source": [
    "Now I want to check the same thing for genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "366c11e074af4e5b8a11cbfa5708c5d7a3876a07"
   },
   "outputs": [],
   "source": [
    "donut_chart(data[\"Genre\"], palette=\"muted\")\n",
    "plt.title(\"Genres\")\n",
    "plt.axis(\"equal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1df62c0152190447314c8816f1a9a1440a23598"
   },
   "source": [
    "The distribution seems ok, even though there is a significant number of different genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5556d60d58fef361ad3ad86cc68ea79e1df07453"
   },
   "outputs": [],
   "source": [
    "scored[\"Grouped_Platform\"] = scored[\"Platform\"].apply(lambda x: get_group_label(x, platforms))\n",
    "donut_chart(scored[\"Grouped_Platform\"])\n",
    "plt.title(\"Groups of platforms for games with score\")\n",
    "plt.axis(\"equal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6456417c6d5cd62b03b13a514f48214bc2bd6f63"
   },
   "source": [
    "Almost all games that have scores are for \"big\" platfroms: PC, PS, Xbox or portable. But there are few from the \"Other\" group. I was interested what these games were so I searched for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a69ac7f0bd5e9939771d2e4837ae8ee15c6315c7"
   },
   "outputs": [],
   "source": [
    "scored[scored[\"Grouped_Platform\"]==\"Other\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a7e8514439d519c4d9005c7222af9175f2308ba5"
   },
   "source": [
    "All these games are for \"DC\" platfom which is Sega Dreamcast, the last of Sega consoles. It was released in 1998 and was the first of sixth generation consoles,  PS2, Gamecube and Xbox. Dreamcast was actually a very good and innovative  product which recieved a lot of positive credit, but it couldn't compete with Sony or Microsoft consoles and Sega was forced to stop the production.\n",
    "\n",
    "   In 2006 Sega started a new wave of sales of Dreamcast consoles and games, which were restored from the leftovers of first production. Following this, IGN re-launched their IGN Dreamcast section to review these games and compare them with PS3, Xbox 360 and Wii games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d707cf6a74a107184fa8796f713d7f296f0b9c8e"
   },
   "source": [
    "Next I want to create some new features: weighted score and my own developer rating. First, I find percent of all games created by each developer, then calculate cumulative percent starting with devs with the least number of games. Finally, I divide them into 5 groups (20% each). Higher rank means more games developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d0e52c091b47d3cee5b716d138966e593266645"
   },
   "outputs": [],
   "source": [
    "scored[\"Weighted_Score\"] = (scored[\"User_Score\"] * 10 * scored[\"User_Count\"] + \n",
    "                            scored[\"Critic_Score\"] * scored[\"Critic_Count\"]) / (scored[\"User_Count\"] + scored[\"Critic_Count\"])\n",
    "devs = pd.DataFrame({\"dev\": scored[\"Developer\"].value_counts().index,\n",
    "                     \"count\": scored[\"Developer\"].value_counts().values})\n",
    "m_score = pd.DataFrame({\"dev\": scored.groupby(\"Developer\")[\"Weighted_Score\"].mean().index,\n",
    "                        \"mean_score\": scored.groupby(\"Developer\")[\"Weighted_Score\"].mean().values})\n",
    "devs = pd.merge(devs, m_score, on=\"dev\")\n",
    "devs = devs.sort_values(by=\"count\", ascending=True)\n",
    "devs[\"percent\"] = devs[\"count\"] / devs[\"count\"].sum()\n",
    "devs[\"top%\"] = devs[\"percent\"].cumsum() * 100\n",
    "n_groups = 5\n",
    "devs[\"top_group\"] = (devs[\"top%\"] * n_groups) // 100 + 1\n",
    "devs[\"top_group\"].iloc[-1] = n_groups\n",
    "devs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ebc479275269a8dd65092693400634fa056a631"
   },
   "source": [
    "A nice graph to see the realtion between developer rank and mean weighted score of developer's games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "432ab23a059b1232e238fa24b9606b796d606537"
   },
   "outputs": [],
   "source": [
    "pal = sns.color_palette(\"RdYlGn\", n_groups)\n",
    "g = sns.JointGrid(x=\"top%\", y=\"mean_score\", data=devs, size=12)\n",
    "legend_elements = []\n",
    "for k in range(0, n_groups):\n",
    "    g.ax_joint.scatter(devs[devs[\"top_group\"]==k+1][\"top%\"], \n",
    "                       devs[devs[\"top_group\"]==k+1][\"mean_score\"],\n",
    "                       color=pal[k], alpha=.9, edgecolor=\"black\")\n",
    "    legend_elements.append(Line2D([0], [0], label=k+1, marker=\"o\", ls=\"\", \n",
    "                                  mfc=pal[k], mec=pal[k], alpha=.9, markersize=15))\n",
    "    g.ax_marg_x.bar(np.arange(k * 100 / n_groups, (k+1) * 100 / n_groups), \n",
    "                    devs[devs[\"top_group\"]==k+1].shape[0], \n",
    "                    width=1, align=\"edge\", color=pal[k], alpha=.9)\n",
    "g.ax_marg_y.hist(devs[\"mean_score\"], color=pal[-1], alpha=.9,\n",
    "                 orientation=\"horizontal\", bins=25, edgecolor=\"white\")\n",
    "g.set_axis_labels(\"Top %\", \"Mean Weighted Score\")\n",
    "g.ax_joint.tick_params(labelsize=15)\n",
    "g.ax_marg_x.grid(False)\n",
    "g.ax_marg_y.grid(False)\n",
    "#g.ax_joint.legend(handles=legend_elements, title=\"Top Group\", loc=4)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle(\"Developers\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6e054217de16ec9160d019c9d1468f662c22d8da"
   },
   "source": [
    "Before creating and fitting a model I have to fill in missing values. I am filling scores and counts with zeros, because there were no real zero scores or counts in the dataset, so it will indicate absence of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60b1ad0abb6b3c2f75a8a9cf0792aef143d583c9",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data[\"Critic_Score\"].fillna(0.0, inplace=True)\n",
    "data[\"Critic_Count\"].fillna(0.0, inplace=True)\n",
    "data[\"User_Score\"].fillna(0.0, inplace=True)\n",
    "data[\"User_Count\"].fillna(0.0, inplace=True)\n",
    "data = data.join(devs.set_index(\"dev\")[\"top_group\"], on=\"Developer\")\n",
    "data = data.rename(columns={\"top_group\": \"Developer_Rank\"})\n",
    "data[\"Developer_Rank\"].fillna(0.0, inplace=True)\n",
    "data[\"Rating\"].fillna(\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d28f816d57cebcd13cafeae39a4b70adb6585bec"
   },
   "source": [
    "Removing outliers in User_Count column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d98bd41480feddc89fd8f658c67ca1f3829610aa",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tmp, rmvd_tmp = rm_outliers(data[data[\"User_Count\"] != 0], [\"User_Count\"])\n",
    "data.drop(rmvd_tmp.index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e0f6bdbc287abd4606fef4869bcee34fd96c91c5"
   },
   "source": [
    "Creating Weighted_Score column (earlier I did it for \"scored\" dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "811114b41e89a8266f1ecf439963047efc77a947",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data[\"Weighted_Score\"] = (data[\"User_Score\"] * 10 * data[\"User_Count\"] + \n",
    "                            data[\"Critic_Score\"] * data[\"Critic_Count\"]) / (data[\"User_Count\"] + data[\"Critic_Count\"])\n",
    "data[\"Weighted_Score\"].fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7efb2301e624acbd74a47cf53c272a3beff0ccba"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ec51ef791f804a9a406e4311acdc5e4bd0a0b26"
   },
   "source": [
    "Now I will do the same things as I did in the basic model, except for using Ordinal encoding for categorical values instead of OneHot. I tried different kinds of encodings, and ordinal seems to work best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9a09ea163662d977ba980de3bb0e82396eecc4b",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "# Select the numeric columns\n",
    "numeric_subset = data.select_dtypes(\"number\").drop(columns=[\"NA\", \"EU\", \"JP\", \"Other\", \"Year\"])\n",
    "\n",
    "# Select the categorical columns\n",
    "categorical_subset = data[[\"Grouped_Platform\", \"Genre\", \"Rating\"]]\n",
    "\n",
    "# One hot encode\n",
    "# categorical_subset = pd.get_dummies(categorical_subset)\n",
    "\n",
    "mapping = []\n",
    "for cat in categorical_subset.columns:\n",
    "    tmp = scored.groupby(cat).median()[\"Weighted_Score\"]\n",
    "    mapping.append({\"col\": cat, \"mapping\": [x for x in np.argsort(tmp).items()]})\n",
    "    \n",
    "encoder = ce.ordinal.OrdinalEncoder()\n",
    "categorical_subset = encoder.fit_transform(categorical_subset, mapping=mapping)\n",
    "\n",
    "# Join the two dataframes using concat\n",
    "# Make sure to use axis = 1 to perform a column bind\n",
    "features = pd.concat([numeric_subset, categorical_subset], axis = 1)\n",
    "\n",
    "# Find correlations with the score \n",
    "correlations = features.corr()[\"Global\"].dropna().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ccbba8050ea5eb99ddee7a518033ba9568db999c"
   },
   "outputs": [],
   "source": [
    "target = pd.Series(features[\"Global\"])\n",
    "features = features.drop(columns=\"Global\")\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, \n",
    "                                                                            test_size=0.2,\n",
    "                                                                            random_state=42)\n",
    "baseline_guess = np.median(target_train)\n",
    "baseline_mae = mae(target_test, baseline_guess)\n",
    "print(\"Baseline guess for global sales is: {:.02f}\".format(baseline_guess))\n",
    "print(\"Baseline Performance on the test set: MAE = {:.04f}\".format(baseline_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0513cb1124d57c5584e992860a708fb3ecdcb0bf"
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(random_state = 42)\n",
    "\n",
    "random_cv = RandomizedSearchCV(estimator=model,\n",
    "                               param_distributions=hyperparameter_grid,\n",
    "                               cv=4, n_iter=20, \n",
    "                               scoring=\"neg_mean_absolute_error\",\n",
    "                               n_jobs=-1, verbose=1, \n",
    "                               return_train_score=True,\n",
    "                               random_state=42)\n",
    "random_cv.fit(features_train, target_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c375748b8d497d6428eb1278de11b9a6986fd674"
   },
   "outputs": [],
   "source": [
    "trees_grid = {\"n_estimators\": [50, 100, 150, 200, 250, 300]}\n",
    "\n",
    "model = random_cv.best_estimator_\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=trees_grid, cv=4, \n",
    "                           scoring=\"neg_mean_absolute_error\", verbose=1,\n",
    "                           n_jobs=-1, return_train_score=True)\n",
    "grid_search.fit(features_train, target_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "43cb2156437a9709dd417fd663980a745e491aa5"
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "plt.plot(results[\"param_n_estimators\"], -1 * results[\"mean_test_score\"], label = \"Testing Error\")\n",
    "plt.plot(results[\"param_n_estimators\"], -1 * results[\"mean_train_score\"], label = \"Training Error\")\n",
    "plt.xlabel(\"Number of Trees\"); plt.ylabel(\"Mean Abosolute Error\"); plt.legend();\n",
    "plt.title(\"Performance vs Number of Trees\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c7a92731a3cef752b147c71cc58665a90d6876f"
   },
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "final_pred = final_model.predict(features_test)\n",
    "final_mae = mae(target_test, final_pred)\n",
    "print(\"Final model performance on the test set: MAE = {:.04f}.\".format(final_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cdc60ec4ff2b67e88f6b66e190eae02c231c725b"
   },
   "source": [
    "\"Advanced\" model gives better results (lower error on test set) which is a good achievement, but the model is still overfitting (graph above). There is definitely room for improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e264c2b2e7a24d861b1e9beef19fd0196ed81a76"
   },
   "source": [
    "And to finish with the project, a nice group of plots summarizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "59106d07e79e3d7c89951aacdb1d6aef4b110436"
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "figsize(20, 16)\n",
    "\n",
    "fig = plt.figure()\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "plt.suptitle(\"Video Games - Predicting Global Sales\", size=30, weight=\"bold\");\n",
    "\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "plt.sca(ax)\n",
    "sns.kdeplot(final_pred, color=\"limegreen\", label=\"Advanced Model\")\n",
    "sns.kdeplot(basic_final_pred, color=\"indianred\", label=\"Basic Model\")\n",
    "sns.kdeplot(target_test, color=\"royalblue\", label=\"Test\")\n",
    "plt.xlabel(\"Global Sales, $M\", size=20); plt.ylabel(\"Density\", size=20);\n",
    "plt.title(\"Distribution of Target Values\", size=24);\n",
    "\n",
    "residuals = final_pred - target_test\n",
    "ax = fig.add_subplot(gs[0, 1])\n",
    "plt.sca(ax)\n",
    "sns.kdeplot(residuals, color = \"limegreen\", label=\"Advanced Model\")\n",
    "sns.kdeplot(basic_residuals, color=\"indianred\", label=\"Basic Model\")\n",
    "plt.xlabel(\"Residuals, $M\", size=20);plt.ylabel(\"Density\", size=20);\n",
    "plt.title(\"Distribution of Errors\", size=24);\n",
    "\n",
    "feature_importance = final_model.feature_importances_\n",
    "feature_names = features.columns.tolist()\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "plt.sca(ax)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center', color=\"goldenrod\")\n",
    "plt.yticks(pos, [feature_names[x] for x in sorted_idx], size=16)\n",
    "plt.xlabel('Relative Importance', size=20)\n",
    "plt.title('Variable Importance', size=24);\n",
    "\n",
    "model_comparison = pd.DataFrame({\"model\": [\"Baseline\", \"Basic\", \"Advanced\"],\n",
    "                                 \"mae\": [basic_baseline_mae, basic_final_mae, final_mae],\n",
    "                                 \"diff\": [\"0.00%\", \"-{:.2f}%\".format((1 - basic_final_mae / basic_baseline_mae) * 100), \"-{:.2f}%\".format((1 - final_mae / basic_baseline_mae) * 100)],\n",
    "                                 \"color\": [\"royalblue\", \"indianred\", \"limegreen\"]})\n",
    "model_comparison.sort_values(\"mae\", ascending=False)\n",
    "pos = np.arange(3) + .5\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "plt.sca(ax)\n",
    "plt.barh(pos, model_comparison[\"mae\"], align=\"center\", color=model_comparison[\"color\"])\n",
    "for i in [1, 2]:\n",
    "    plt.text(plt.xlim()[1], pos[i], model_comparison[\"diff\"][i], \n",
    "             verticalalignment=\"center\", horizontalalignment=\"right\")\n",
    "plt.yticks(pos, model_comparison[\"model\"], size=16); plt.xlabel(\"Mean Absolute Error\", size=20);\n",
    "plt.title(\"Test MAE\", size=24);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2f8992c5c2880d2e59c1fce38ac521b84dba9fc1",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
